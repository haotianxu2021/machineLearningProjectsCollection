# Machine Learning Projects Collection

## Adversarial Example
 In this project, we investigate techniques for creating adversarial examples aimed at neural networks, specifically using the MNIST dataset
 and targeting a pretrained multi-layer percep
tron (MLP) with three hidden layers. Our ex
ploration starts with the Fast Gradient Sign
 Method (FGSM) as a foundational approach.
 Building on this, we examine two variations:
 the Basic Iterative Method and the Target Class
 Method. Additionally, we develop and evaluate
 a novel method inspired by these established
 techniques, adapting the FGSM framework to
 enhance its efficacy

## Coordinate Descent
 In this project, we explore how well different
 coordinate descent methods work for solving
 binary logistic regression, a common type of
 problem in machine learning for classifying
 data into two categories. We compare three
 methods: Random-feature Coordinate Descent
 (RCD), Greedy Coordinate Descent (GCD),
 and Cyclic Coordinate Descent (CCD). Our
 goal is to see how each method chooses coor
dinates and updates them, and then test to see
 which one performs best through experiments.

## Play prediction
In this project, we predict whether players would play certain games and how many hours they would play using the dataset from Steam. We use Bayesian Personalized Ranking and some other similarity metrics as features, and use Logistic Regression to predict whether players would play games (We also explore other models and use the best one). We use Latent Factor Model to predict the time players would play games. 

## Prototype Selection
 In this project, we investigate the efficacy of
 prototype selection methods in enhancing the
 speed of nearest neighbor classification. Specif
ically, we implement and compare three dis
tinct approaches: Random Selection, K-means
 clustering, and Condensed Nearest Neighbor
 (CNN). Our evaluation focuses on their perfor
mance relative to classification accuracy and
 computational efficiency when applied to the
 MNIST dataset. This comparative analysis
 aims to determine the most effective technique
 for reducing computational demands while
 maintaining or improving the accuracy of the
 nearest neighbor classifier

## Stacking
Using
 Stacking to Enhance Model Performances
 on Rating Predictions for Food Recipe.
